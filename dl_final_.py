# -*- coding: utf-8 -*-
"""DL_Final_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1InYb9uiSOgOjbuodVgdPiIMfSNqhnZC1
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = np.repeat(X_train[..., np.newaxis], 3, -1)
X_train = tf.image.resize(X_train, [64, 64]).numpy()
X_test = np.repeat(X_test[..., np.newaxis], 3, -1)
X_test = tf.image.resize(X_test, [64, 64]).numpy()
X_train, X_test = X_train / 255.0, X_test / 255.0

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))
for layer in vgg_model.layers[:10]:

model = models.Sequential([
    vgg_model,
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

datagen = ImageDataGenerator()
train_generator = datagen.flow(X_train, y_train, batch_size=32)
test_generator = datagen.flow(X_test, y_test, batch_size=32)

   history = model.fit(train_generator, epochs=10, validation_data=test_generator)

import matplotlib.pyplot as plt

test_loss, test_acc = model.evaluate(test_generator, verbose=2)
print(f"\nTest accuracy: {test_acc:.2f}")
print(f"Test loss: {test_loss:.2f}")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()









"""#3. Implement the backpropagation algorithm from scratch and verify its correctness.

#2. Build a multi-layer feedforward neural network to solve a classification problem.

#4. Visualize how the weights of a neural network change during training.

## Self - instance of the class itself.
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.utils import to_categorical

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train.reshape(-1, 28*28).astype('float32') / 255
X_test = X_test.reshape(-1, 28*28).astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Activation functions and their derivatives
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Neural Network class with backpropagation
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01
        self.bias_hidden = np.zeros((1, hidden_size))
        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01
        self.bias_output = np.zeros((1, output_size))

        # For plotting
        self.weight_history_input_hidden = []
        self.weight_history_hidden_output = []

    def forward(self, X):
        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden
        self.hidden_output = sigmoid(self.hidden_input)
        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.final_output = softmax(self.final_input)
        return self.final_output

    def backward(self, X, y, output):
        error_output = output - y
        delta_output = error_output

        error_hidden = delta_output.dot(self.weights_hidden_output.T)
        delta_hidden = error_hidden * sigmoid_derivative(self.hidden_output)

        # Update weights and biases
        self.weights_hidden_output -= self.learning_rate * self.hidden_output.T.dot(delta_output)
        self.bias_output -= self.learning_rate * np.sum(delta_output, axis=0, keepdims=True)
        self.weights_input_hidden -= self.learning_rate * X.T.dot(delta_hidden)
        self.bias_hidden -= self.learning_rate * np.sum(delta_hidden, axis=0, keepdims=True)

    def train(self, X, y, epochs=20, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, X.shape[0], batch_size):
                X_batch = X[i:i+batch_size]
                y_batch = y[i:i+batch_size]
                output = self.forward(X_batch)
                self.backward(X_batch, y_batch, output)

            if epoch % 5 == 0:  # Record weights every 5 epochs
                self.weight_history_input_hidden.append(self.weights_input_hidden.copy())
                self.weight_history_hidden_output.append(self.weights_hidden_output.copy())
                loss = -np.mean(np.sum(y_batch * np.log(output + 1e-8), axis=1))
                print(f'Epoch {epoch}/{epochs}, Loss: {loss}')

    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)

    def plot_weights(self):
        # Plot the weight changes for the first 10 epochs
        epochs_to_plot = min(10, len(self.weight_history_input_hidden))
        plt.figure(figsize=(12, 6))

        plt.subplot(1, 2, 1)
        plt.title('Weights (Input to Hidden)')
        for epoch in range(epochs_to_plot):
            plt.plot(self.weight_history_input_hidden[epoch].flatten(), label=f'Epoch {epoch*5}')
        plt.xlabel('Weight Index')
        plt.ylabel('Weight Value')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.title('Weights (Hidden to Output)')
        for epoch in range(epochs_to_plot):
            plt.plot(self.weight_history_hidden_output[epoch].flatten(), label=f'Epoch {epoch*5}')
        plt.xlabel('Weight Index')
        plt.ylabel('Weight Value')
        plt.legend()

        plt.tight_layout()
        plt.show()

# Hyperparameters
input_size = 28 * 28
hidden_size = 128
output_size = 10
learning_rate = 0.01
epochs = 20
batch_size = 32

# Initialize and train the neural network
nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)
nn.train(X_train, y_train, epochs, batch_size)

# Evaluate the model on the test set
predictions = nn.predict(X_test)
accuracy = np.mean(predictions == np.argmax(y_test, axis=1))
print(f'Test Accuracy: {accuracy * 100}%')

# Plot the weights
nn.plot_weights()

"""#5. Building a simple neural network using iris dataset"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Preprocess the data
# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the neural network model
model = Sequential()
model.add(Dense(10, input_shape=(4,), activation='relu'))  # Input layer with 10 neurons and ReLU activation
model.add(Dense(10, activation='relu'))                    # Hidden layer with 10 neurons and ReLU activation
model.add(Dense(3, activation='softmax'))                  # Output layer with 3 neurons (for 3 classes) and softmax activation

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
epochs = 20
batch_size = 32
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

"""#1. Implement a single-layer perceptron from scratch without using any neural network libraries.

"""

import matplotlib.pyplot as plt
import numpy as np

def visualize_model():
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.axis('off')

    # Layer configurations
    layer_sizes = [4, 10, 10, 3]  # Input size, hidden layers, and output size

    v_spacing = 1.0  # vertical spacing between layers
    h_spacing = 0.5  # horizontal spacing between nodes in a layer

    # Positions of each neuron
    layer_positions = []
    for i, size in enumerate(layer_sizes):
        layer_positions.append(np.linspace(-0.5 * (size - 1), 0.5 * (size - 1), size))

    # Plot neurons
    for i, layer in enumerate(layer_positions):
        for j, y in enumerate(layer):
            ax.scatter(i * h_spacing, y * v_spacing, s=200, color='b', edgecolor='k', zorder=5)
            ax.text(i * h_spacing, y * v_spacing, f's{i+1}.{j+1}', fontsize=10, ha='center', va='center', color='w')

    # Plot connections between neurons
    for i in range(len(layer_positions) - 1):
        for a in layer_positions[i]:
            for b in layer_positions[i + 1]:
                ax.plot([i * h_spacing, (i + 1) * h_spacing], [a * v_spacing, b * v_spacing], color='k')

    # Layer labels
    ax.text(0 * h_spacing, -5 * v_spacing, 'Input Layer', fontsize=12, ha='left', va='center')
    ax.text(1 * h_spacing, -5 * v_spacing, 'Hidden Layer 1', fontsize=12, ha='left', va='center')
    ax.text(2 * h_spacing, -5 * v_spacing, 'Hidden Layer 2', fontsize=12, ha='left', va='center')
    ax.text(3 * h_spacing, -5 * v_spacing, 'Output Layer', fontsize=12, ha='left', va='center')

    plt.show()

# Call the function to visualize the model
visualize_model()

import numpy as np

def step_function(x):
    return np.where(x > 0, 1, 0)

class Perceptron:
    def __init__(self, input_size, learning_rate=0.01, epochs=1000):
        self.input_size = input_size
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = np.zeros(input_size + 1)  # including the bias term

    def fit(self, X, y):
        for _ in range(self.epochs):
            for i in range(len(y)):
                # Compute the prediction
                prediction = self.predict(X[i])
                # Update weights
                self.weights[1:] += self.learning_rate * (y[i] - prediction) * X[i]
                self.weights[0] += self.learning_rate * (y[i] - prediction)  # Update bias

    def predict(self, x):
        # Compute the weighted sum and apply the step function
        weighted_sum = np.dot(x, self.weights[1:]) + self.weights[0]
        return step_function(weighted_sum)

# AND gate input and output
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # Output of AND gate

# Initialize the Perceptron
perceptron = Perceptron(input_size=2, learning_rate=0.01, epochs=1000)

# Train the Perceptron
perceptron.fit(X, y)

# Make predictions on the training data
print("Training data and predictions:")
for x in X:
    print(f"Input: {x}, Prediction: {perceptron.predict(x)}")

"""###final"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  # Flatten and normalize
X_test = X_test.reshape(X_test.shape[0], -1) / 255.0    # Flatten and normalize

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build the single-layer perceptron model
model = Sequential([
    Dense(10, input_shape=(784,), activation='softmax')
])

# Compile the model
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Make predictions on the test set
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

# Print the first 10 predictions
for i in range(10):
    print(f"True class: {true_classes[i]}, Predicted class: {predicted_classes[i]}")

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
from keras.callbacks import LambdaCallback

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train.reshape(-1, 28*28).astype('float32') / 255
X_test = X_test.reshape(-1, 28*28).astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build the multi-layer feedforward neural network
model = Sequential([
    Dense(128, input_shape=(784,), activation='relu', kernel_initializer='he_uniform'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Function to record weights
weights_history = []

def record_weights(epoch, logs):
    weights_history.append(model.get_weights())

weight_callback = LambdaCallback(on_epoch_end=record_weights)

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, callbacks=[weight_callback])

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')

# Function to plot weights
def plot_weights(weights_history, layer_index, title):
    plt.figure(figsize=(12, 6))
    for epoch, weights in enumerate(weights_history):
        plt.plot(weights[layer_index].flatten(), label=f'Epoch {epoch}')
    plt.xlabel('Weight Index')
    plt.ylabel('Weight Value')
    plt.title(title)
    plt.legend()
    plt.show()

# Plot weights for the first dense layer (input to hidden)
plot_weights(weights_history, 0, 'Weights (Input to Hidden Layer)')

# Plot weights for the second dense layer (hidden to output)
plot_weights(weights_history, 2, 'Weights (Hidden to Output Layer)')

# Backpropagation verification

# Simple manual implementation of forward pass for a single example
def forward_pass(X, weights):
    layer1 = np.maximum(0, np.dot(X, weights[0]) + weights[1])  # ReLU activation
    output = np.exp(np.dot(layer1, weights[2]) + weights[3])
    output = output / np.sum(output, axis=1, keepdims=True)  # Softmax
    return output

# Manual implementation of the loss function (cross-entropy)
def compute_loss(y_true, y_pred):
    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))

# Gradient check
def gradient_check(X, y_true, weights, epsilon=1e-5):
    loss_before = compute_loss(y_true, forward_pass(X, weights))
    for i in range(len(weights)):
        weights_plus = weights.copy()
        weights_plus[i] += epsilon
        loss_plus = compute_loss(y_true, forward_pass(X, weights_plus))
        weights_minus = weights.copy()
        weights_minus[i] -= epsilon
        loss_minus = compute_loss(y_true, forward_pass(X, weights_minus))

        estimated_gradient = (loss_plus - loss_minus) / (2 * epsilon)
        print(f'Gradient check for weight {i}: {estimated_gradient:.5e}')

# Example usage of gradient check (requires proper gradient computation implementation)
# gradient_check(X_train[:10], y_train[:10], model.get_weights())

# Predictions
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

# Print the first 10 predictions
for i in range(10):
    print(f"True class: {true_classes[i]}, Predicted class: {predicted_classes[i]}")

"""# EXP_2: Vanishing *Gradient*

### Task Aim
####Implement and train a neural network on the MNIST dataset using stochastic gradient descent (SGD) to address and reduce the vanishing gradient problem in a multilayer model.

### Learning Outcome
#### The code applies stochastic gradient descent (SGD) on the MNIST dataset to train a neural network, aiming to mitigate the vanishing gradient problem in a multilayer model.
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28 * 28) / 255.0
x_test = x_test.reshape(-1, 28 * 28) / 255.0

# Convert labels to one-hot encoding
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Build the neural network model
model = Sequential([
    Dense(128, activation='relu', input_shape=(28 * 28,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Set the learning rate and optimizer
learning_rate = 0.01
optimizer = SGD(learning_rate=learning_rate)

# Compile the model
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

"""import matplotlib.pyplot as plt
import numpy as np

def visualize_model():
    fig, ax = plt.subplots(figsize=(300, 250))  # Further increase figure size
    ax.axis('off')

    # Layer configurations
    layer_sizes = [784, 128, 64, 10]  # Input size, hidden layers, and output size

    v_spacing = 10  # Increase vertical spacing between layers to reduce clutter
    h_spacing = 6  # Further increase horizontal spacing between nodes in a layer

    # Positions of each neuron
    layer_positions = []
    for i, size in enumerate(layer_sizes):
        layer_positions.append(np.linspace(-0.5 * (size - 1), 0.5 * (size - 1), size))

    # Plot neurons
    for i, layer in enumerate(layer_positions):
        for j, y in enumerate(layer):
            ax.scatter(i * h_spacing, y * v_spacing, s=500, color='b', edgecolor='k', zorder=5)  # Further increase neuron size
            if i == 0:  # Label input layer neurons
                ax.text(i * h_spacing - 0.3, y * v_spacing, f'I{j+1}', fontsize=8, ha='right', va='center', color='k')
            elif i == len(layer_sizes) - 1:  # Label output layer neurons
                ax.text(i * h_spacing + 0.3, y * v_spacing, f'O{j+1}', fontsize=8, ha='left', va='center', color='k')
            else:  # Label hidden layers neurons
                ax.text(i * h_spacing, y * v_spacing, f'{j+1}', fontsize=6, ha='center', va='center', color='w')

    # Plot connections between neurons
    for i in range(len(layer_positions) - 1):
        for a in layer_positions[i]:
            for b in layer_positions[i + 1]:
                ax.plot([i * h_spacing, (i + 1) * h_spacing], [a * v_spacing, b * v_spacing], color='k')""

    # Layer labels at the bottom, aligned to the left
    ax.text(0 * h_spacing, -4 * v_spacing, 'Input Layer', fontsize=14, ha='left', va='center')
    ax.text(1 * h_spacing, -4 * v_spacing, 'Hidden Layer 1', fontsize=14, ha='left', va='center')
    ax.text(2 * h_spacing, -4 * v_spacing, 'Hidden Layer 2', fontsize=14, ha='left', va='center')
    ax.text(3 * h_spacing, -4 * v_spacing, 'Output Layer', fontsize=14, ha='left', va='center')

    plt.show()

# Call the function to visualize the model
visualize_model()

# Exp_3 : Momentum
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28 * 28) / 255.0
x_test = x_test.reshape(-1, 28 * 28) / 255.0

y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

model = Sequential([
    Dense(128, activation='relu', input_shape=(28 * 28,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

learning_rate = 0.01
momentum = 0.9
optimizer = SGD(learning_rate=learning_rate, momentum=momentum)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))

loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

"""# Exp_4: RF iris data and deep learning Feature Extraction

# Iris dataset
- optimizer SGD
- sparse_categorical_crossentropy
- 0, 1, 2 normalization
- Important feature from hidden layers(not from the output layeres) and then fitting them into the keras, tensorflow, SVC and RF
- Modern model and traditional model of the machine learning and deep learning
- Hybrid model of neural network and traditional deep learning will provide the more accuracy
- Fitting and Accuracy Evaluation
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from keras.models import Sequential, Model
from keras.layers import Dense, Input, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
import tensorflow as tf

data = load_iris()
X = data.data
y = data.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

inputs = Input(shape=(4,))
x = Dense(64, activation='relu')(inputs)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(32, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
outputs = Dense(3, activation='softmax')(x)

nn_model = Model(inputs=inputs, outputs=outputs)

nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
epochs = 100
batch_size = 32
history = nn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                       validation_split=0.2, verbose=0, callbacks=[early_stopping])

intermediate_layer_model = Model(inputs=nn_model.input, outputs=nn_model.layers[1].output)
hidden_features_train = intermediate_layer_model.predict(X_train)
hidden_features_test = intermediate_layer_model.predict(X_test)

svc_params = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10, 100]}
svc_model = GridSearchCV(SVC(), svc_params, cv=5)
svc_model.fit(hidden_features_train, y_train)
svc_pred = svc_model.predict(hidden_features_test)
svc_accuracy = accuracy_score(y_test, svc_pred)

rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}
rf_model = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)
rf_model.fit(hidden_features_train, y_train)
rf_pred = rf_model.predict(hidden_features_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test, verbose=0)

print(f'Neural Network Accuracy: {nn_accuracy * 100:.2f}%')
print(f'SVM Accuracy: {svc_accuracy * 100:.2f}%')
print(f'Random Forest Accuracy: {rf_accuracy * 100:.2f}%')

hybrid_accuracy = (nn_accuracy * 0.3 + svc_accuracy * 0.35 + rf_accuracy * 0.35)
print(f'Hybrid Model Accuracy Estimate: {hybrid_accuracy * 100:.2f}%')

"""#Exercise
##Regression problem using NN - House Price Dataset
"""

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
import matplotlib.pyplot as plt

# Load the dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='Price')

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the Neural Network model
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1)  # Output layer for regression
])

# Compile the model using the SGD optimizer
nn_model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='mean_squared_error', metrics=['mae'])

# Train the Neural Network model
nn_history = nn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the Neural Network model on the test set
nn_predictions = nn_model.predict(X_test)
nn_mse = mean_squared_error(y_test, nn_predictions)
nn_mae = mean_absolute_error(y_test, nn_predictions)
nn_r2 = r2_score(y_test, nn_predictions)

print(f"Neural Network - MSE: {nn_mse}, MAE: {nn_mae}, R²: {nn_r2}")

# Train a Linear Regression model
lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train)

# Make predictions using Linear Regression
lin_reg_predictions = lin_reg_model.predict(X_test)
lin_reg_mse = mean_squared_error(y_test, lin_reg_predictions)
lin_reg_mae = mean_absolute_error(y_test, lin_reg_predictions)
lin_reg_r2 = r2_score(y_test, lin_reg_predictions)

print(f"Linear Regression - MSE: {lin_reg_mse}, MAE: {lin_reg_mae}, R²: {lin_reg_r2}")

# Plot training & validation loss for Neural Network
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(nn_history.history['loss'], label='Train Loss')
plt.plot(nn_history.history['val_loss'], label='Validation Loss')
plt.title('Neural Network Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot predictions vs true values for both models
plt.subplot(1, 2, 2)
plt.scatter(y_test, nn_predictions, label='NN Predictions', alpha=0.5)
plt.scatter(y_test, lin_reg_predictions, label='Linear Regression Predictions', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.title('Predictions vs True Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.legend()

plt.tight_layout()
plt.show()

# Show some predictions
print(f"First 5 Neural Network Predictions: {nn_predictions[:5].flatten()}")
print(f"First 5 Linear Regression Predictions: {lin_reg_predictions[:5]}")
print(f"First 5 True Values: {y_test.values[:5]}")

import pandas as pd
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target #To see the dataset

df.head()

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
import matplotlib.pyplot as plt

# Load the dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='Price')

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the Neural Network model
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1)  # Output layer for regression
])

# Compile the model using the SGD optimizer
nn_model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='mean_squared_error', metrics=['mae'])

# Train the Neural Network model
nn_history = nn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

# Evaluate the Neural Network model on the test set
nn_predictions = nn_model.predict(X_test)
nn_mse = mean_squared_error(y_test, nn_predictions)
nn_mae = mean_absolute_error(y_test, nn_predictions)
nn_r2 = r2_score(y_test, nn_predictions)

print(f"Neural Network Metrics:\nMSE: {nn_mse:.4f}, MAE: {nn_mae:.4f}, R²: {nn_r2:.4f}")

# Train a Linear Regression model
lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train)

# Make predictions using Linear Regression
lin_reg_predictions = lin_reg_model.predict(X_test)
lin_reg_mse = mean_squared_error(y_test, lin_reg_predictions)
lin_reg_mae = mean_absolute_error(y_test, lin_reg_predictions)
lin_reg_r2 = r2_score(y_test, lin_reg_predictions)

print(f"\nLinear Regression Metrics:\nMSE: {lin_reg_mse:.4f}, MAE: {lin_reg_mae:.4f}, R²: {lin_reg_r2:.4f}")

# Plot training & validation loss for Neural Network
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(nn_history.history['loss'], label='Train Loss')
plt.plot(nn_history.history['val_loss'], label='Validation Loss')
plt.title('Neural Network Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot predictions vs true values for both models
plt.subplot(1, 2, 2)
plt.scatter(y_test, nn_predictions, label='NN Predictions', alpha=0.5)
plt.scatter(y_test, lin_reg_predictions, label='Linear Regression Predictions', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.title('Predictions vs True Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.legend()

plt.tight_layout()
plt.show()

# Show some predictions
print(f"\nFirst 5 Neural Network Predictions: {nn_predictions[:5].flatten()}")
print(f"First 5 Linear Regression Predictions: {lin_reg_predictions[:5]}")
print(f"First 5 True Values: {y_test.values[:5]}")

"""#Exp: 6th CNN Padding

### Padding 'same' maintains the spatial dimensions after convolution.
### Padding 'valid' reduces the dimensions, and combining it with stride = 2 further reduces the size of the feature maps.
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0

train_images = train_images.reshape(-1, 28, 28, 1)
test_images = test_images.reshape(-1, 28, 28, 1)

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same', strides=1),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=1),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation='relu', padding='valid', strides=2),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),

    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))

test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#EXP-5: Implement a simple feed forward neural network network with Adagrad optimizer."""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adagrad
import numpy as np


model = Sequential()
model.add(Dense(units=128, activation='relu', input_shape=(32,)))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=3, activation='softmax'))


adagrad = Adagrad(learning_rate=0.01)
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()


X_train = np.random.rand(1000, 32)
y_train = np.random.randint(3, size=(1000,))

model.fit(X_train, y_train, epochs=10, batch_size=32)


X_test = np.random.rand(200, 32)
y_test = np.random.randint(3, size=(200,))


loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

"""### Above Low Accuracy: Adagrad

Adagrad with mnist
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adagrad
from tensorflow.keras.utils import to_categorical

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train / 255.0
X_test = X_test / 255.0

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=10, activation='softmax'))

adagrad = Adagrad(learning_rate=0.01)
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

import matplotlib.pyplot as plt

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adagrad

(X_train, y_train), (X_test, y_test) = mnist.load_data()


X_train = X_train / 255.0
X_test = X_test / 255.0

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))
model.add(Dense(units=10, activation='softmax'))

adagrad = Adagrad(learning_rate=0.01)
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

"""#EXP-7 : Implement Greedy Layer wise pre-trainig in neural network."""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import fashion_mnist

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

X_train = X_train.reshape(-1, 28*28).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28*28).astype('float32') / 255.0

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

mlp = keras.models.Sequential()
mlp.add(keras.layers.Dense(units=512, activation='tanh', kernel_initializer='he_uniform', input_shape=(28*28,)))
mlp.add(keras.layers.Dense(units=10, activation='softmax', kernel_initializer='he_uniform'))
mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

mlp.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

output_layer = mlp.layers[-1]

mlp.pop()
for layer in mlp.layers:
    layer.trainable = False

mlp.add(keras.layers.Dense(units=512, activation='tanh', kernel_initializer='he_uniform'))
mlp.add(output_layer)

mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

mlp.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))

test_loss, test_accuracy = mlp.evaluate(X_test, y_test)

print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""#Exp-8: Transfer Learning"""

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

X_train, X_test = X_train / 255.0, X_test / 255.0

#(28, 28, 1)
X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)

















class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.binary)
    plt.xlabel(class_names[y_train[i]])
plt.show()

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_test, y_test))

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"\nTest accuracy: {test_acc:.2f}")

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

import numpy as np
predictions = model.predict(X_test)

plt.figure()
plt.imshow(X_test[0].reshape(28, 28), cmap=plt.cm.binary)
plt.title(f"Predicted: {class_names[np.argmax(predictions[0])]}, True: {class_names[y_test[0]]}")
plt.show()

"""##implementation"""

import tensorflow as tf
import numpy as np

fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)

X_train = tf.image.resize(X_train, (224, 224))
X_test = tf.image.resize(X_test, (224, 224))

X_train = np.repeat(X_train, 3, axis=-1)
X_test = np.repeat(X_test, 3, axis=-1)

X_train, X_test = X_train / 255.0, X_test / 255.0

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(10, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"Test accuracy: {test_acc:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

import numpy as np

predictions = model.predict(X_test)

plt.figure()
plt.imshow(X_test[0].reshape(224, 224, 3))
plt.title(f"Predicted: {np.argmax(predictions[0])}, True: {y_test[0]}")
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load and preprocess the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize and reshape data
x_train = x_train / 255.0
x_test = x_test / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Create a simple CNN model
def create_cnn_model():
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(10, activation='softmax')  # 10 classes for Fashion MNIST
    ])
    return model

# Compile the CNN model
cnn_model = create_cnn_model()
cnn_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the CNN model
cnn_history = cnn_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))

# Evaluate the CNN model
cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(x_test, y_test)

print(f'CNN Test Accuracy: {cnn_test_accuracy * 100:.2f}%')

# Convert grayscale images to RGB to fit MobileNetV2 requirements
x_train_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_train))
x_test_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_test))

# Resize images to 32x32 to fit MobileNetV2 input requirements
x_train_resized = tf.image.resize(x_train_rgb, (32, 32))
x_test_resized = tf.image.resize(x_test_rgb, (32, 32))

# Use MobileNetV2 as a feature extractor for transfer learning
base_model = MobileNetV2(input_shape=(32, 32, 3), include_top=False, weights='imagenet')
base_model.trainable = False  # Freeze base model layers

# Build a model for transfer learning
def create_transfer_learning_model():
    model = Sequential([
        base_model,
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(10, activation='softmax')  # 10 classes for Fashion MNIST
    ])
    return model

# Compile the transfer learning model
transfer_model = create_transfer_learning_model()
transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the transfer learning model
transfer_history = transfer_model.fit(x_train_resized, y_train, epochs=10, batch_size=32, validation_data=(x_test_resized, y_test))

# Evaluate the transfer learning model
transfer_test_loss, transfer_test_accuracy = transfer_model.evaluate(x_test_resized, y_test)

print(f'Transfer Learning Test Accuracy: {transfer_test_accuracy * 100:.2f}%')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications import EfficientNetB0

# Enable mixed precision for memory optimization (optional)
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

# Load and preprocess the Fashion MNIST dataset
fashion_mnist = tf.keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize and reshape the data
x_train = x_train / 255.0
x_test = x_test / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Convert grayscale images to RGB to fit EfficientNet input requirements
x_train_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_train))
x_test_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_test))

# Resize images to 128x128 to reduce memory usage and fit EfficientNet input size
x_train_resized = tf.image.resize(x_train_rgb, (128, 128))
x_test_resized = tf.image.resize(x_test_rgb, (128, 128))

# Load EfficientNetB0 model with pre-trained weights (without top layers)
base_model = EfficientNetB0(input_shape=(128, 128, 3), include_top=False, weights='imagenet')
base_model.trainable = False  # Freeze the pre-trained layers

# Build the transfer learning model
model = Sequential([
    base_model,
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')  # 10 classes for Fashion MNIST
])

# Compile the model with 'adam' optimizer and sparse categorical crossentropy loss
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model with a reduced batch size (e.g., 8) to save memory
history = model.fit(x_train_resized, y_train, epochs=10, batch_size=8, validation_data=(x_test_resized, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(x_test_resized, y_test)

print(f'EfficientNetB0 Transfer Learning Test Accuracy: {test_accuracy * 100:.2f}%')

"""#EXP_6_DL_FINAL"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt

# Load and preprocess the data
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0

# Reshape the data to add the color channel
train_images = train_images.reshape(-1, 28, 28, 1)
test_images = test_images.reshape(-1, 28, 28, 1)

# Define the CNN model
model = models.Sequential([
    # First convolutional block with 'same' padding and max pooling
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same', strides=1),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Second convolutional block with 'same' padding and max pooling
    layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=1),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),

    # Third convolutional block with 'valid' padding and global average pooling
    layers.Conv2D(128, (3, 3), activation='relu', padding='valid', strides=1),
    layers.BatchNormalization(),

    # Apply Global Average Pooling instead of Flattening
    layers.GlobalAveragePooling2D(),

    # Output layer for classification
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

